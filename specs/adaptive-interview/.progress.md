# Progress: adaptive-interview

## Original Goal

It seems like the interview questions are too specific and never changing. They should be adaptive and dynamic based on the conversation and the other specs

## Goal Context

Interview responses from goal clarification:
- Problem: Fixing a bug or issue
- Constraints: No special constraints
- Success criteria: Questions adapt based on context
- Bug details:
  - Same questions appear regardless of context
  - Questions don't consider existing specs
  - Questions are too generic/not useful

## Status

- Phase: execution
- Started: 2026-01-22

## Completed Tasks
- [x] 1.1 Add intent classifier to start.md Goal Interview
- [x] 1.2 Convert Goal Interview to single-question flow
- [x] 1.3 Add question classification instructions
- [x] 1.4 Store interview responses in .progress.md

## Learnings

- Parameter chain pattern is ideal for goal clarification - check "do we have X?" before asking
- Question piping (inserting prior answers into questions) creates personalized feel
- Max 5 choices per question - users forget middle options
- Rule-based branching for known paths, AI-generated for probing depth - hybrid approach best
- Always explain "why" before asking for information - frames value for user
- Avoid open text as branching triggers - use closed-ended questions
- Context awareness requires explicit conversation state management
- Best practice: core template questions + conditional branches + AI follow-ups for depth
- 5 interview locations: start.md, research.md, requirements.md, design.md, tasks.md - each has same adaptive depth pattern
- Task 1.1: Intent Classification section added with 4 intent types (TRIVIAL, REFACTOR, GREENFIELD, MID_SIZED) and question ranges
- All interviews use identical "Other" follow-up template - opportunity for option-specific branching
- Phase artifacts available but not currently read by interviews - key gap to address
- Related specs in ./specs/ never consulted - major context source being ignored
- Hybrid approach (rule-based + AI) chosen over pure AI generation for testability
- Design: Simple {var} piping syntax chosen over Handlebars - matches existing patterns, less escaping
- Design: Single .progress.md stores all interview responses vs separate files - simpler accumulation
- Design: Keyword-based spec matching (not AI) for related specs - testable, predictable
- Design: Hard skip for parameter chain (not soft/disabled) - cleaner UX
- Design: Max 3 related specs shown to prevent overwhelm
- Design: Spec scanner reads only .progress.md per spec (not full artifacts) for performance
- Design: All changes in command files only - no agent modifications needed
- Design: Question-to-key mapping defined per phase for parameter chain checking
- Design: Fallback to original question if piping variable not found - graceful degradation
- Tasks: POC-first approach - start.md is the entry point, get it working there first, then propagate
- Task 1.3: Question Classification matrix distinguishes codebase facts (use Explore) from user decisions (use AskUserQuestion)
- Tasks: 23 total tasks across 4 phases (7 POC + 6 propagate + 6 enhance + 4 quality gates)
- Tasks: Verification is limited to file validation and manual plugin testing (no Jest/E2E in this plugin)
- Tasks: Quality checkpoints every 2-3 tasks using grep/head commands for file validation
- Tasks: Branch already exists (feat/adaptive-interview) - no branch creation needed in Phase 4
- Task 1.2: Single-question flow pattern uses loop with askedCount/minRequired/maxAllowed tracking
- Task 1.2: Completion signal detection checks for user intent words ("done", "proceed", etc.) after minRequired reached
- Task 1.2: Each question stored in responses object with semantic key (problem, constraints, success, additionalContext)
- Task 1.4: Context accumulator stores intent classification + interview responses in .progress.md with parseable format for parameter chain
