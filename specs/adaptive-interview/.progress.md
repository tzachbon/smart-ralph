# Progress: adaptive-interview

## Original Goal

It seems like the interview questions are too specific and never changing. They should be adaptive and dynamic based on the conversation and the other specs

## Goal Context

Interview responses from goal clarification:
- Problem: Fixing a bug or issue
- Constraints: No special constraints
- Success criteria: Questions adapt based on context
- Bug details:
  - Same questions appear regardless of context
  - Questions don't consider existing specs
  - Questions are too generic/not useful

## Status

- Phase: execution
- Started: 2026-01-22

## Completed Tasks
- [x] 1.1 Add intent classifier to start.md Goal Interview
- [x] 1.2 Convert Goal Interview to single-question flow
- [x] 1.3 Add question classification instructions
- [x] 1.4 Store interview responses in .progress.md
- [x] 1.5 [VERIFY] Quality checkpoint: verify start.md is valid markdown
- [x] 1.6 Define question pools per intent type
- [x] 1.7 POC Checkpoint: verify single-question flow works in start.md
- [x] 2.1 Add single-question flow to research.md

## Learnings

- Parameter chain pattern is ideal for goal clarification - check "do we have X?" before asking
- Question piping (inserting prior answers into questions) creates personalized feel
- Max 5 choices per question - users forget middle options
- Rule-based branching for known paths, AI-generated for probing depth - hybrid approach best
- Always explain "why" before asking for information - frames value for user
- Avoid open text as branching triggers - use closed-ended questions
- Context awareness requires explicit conversation state management
- Best practice: core template questions + conditional branches + AI follow-ups for depth
- 5 interview locations: start.md, research.md, requirements.md, design.md, tasks.md - each has same adaptive depth pattern
- Task 1.1: Intent Classification section added with 4 intent types (TRIVIAL, REFACTOR, GREENFIELD, MID_SIZED) and question ranges
- All interviews use identical "Other" follow-up template - opportunity for option-specific branching
- Phase artifacts available but not currently read by interviews - key gap to address
- Related specs in ./specs/ never consulted - major context source being ignored
- Hybrid approach (rule-based + AI) chosen over pure AI generation for testability
- Design: Simple {var} piping syntax chosen over Handlebars - matches existing patterns, less escaping
- Design: Single .progress.md stores all interview responses vs separate files - simpler accumulation
- Design: Keyword-based spec matching (not AI) for related specs - testable, predictable
- Design: Hard skip for parameter chain (not soft/disabled) - cleaner UX
- Design: Max 3 related specs shown to prevent overwhelm
- Design: Spec scanner reads only .progress.md per spec (not full artifacts) for performance
- Design: All changes in command files only - no agent modifications needed
- Design: Question-to-key mapping defined per phase for parameter chain checking
- Design: Fallback to original question if piping variable not found - graceful degradation
- Tasks: POC-first approach - start.md is the entry point, get it working there first, then propagate
- Task 1.3: Question Classification matrix distinguishes codebase facts (use Explore) from user decisions (use AskUserQuestion)
- Tasks: 23 total tasks across 4 phases (7 POC + 6 propagate + 6 enhance + 4 quality gates)
- Tasks: Verification is limited to file validation and manual plugin testing (no Jest/E2E in this plugin)
- Tasks: Quality checkpoints every 2-3 tasks using grep/head commands for file validation
- Tasks: Branch already exists (feat/adaptive-interview) - no branch creation needed in Phase 4
- Task 1.2: Single-question flow pattern uses loop with askedCount/minRequired/maxAllowed tracking
- Task 1.2: Completion signal detection checks for user intent words ("done", "proceed", etc.) after minRequired reached
- Task 1.2: Each question stored in responses object with semantic key (problem, constraints, success, additionalContext)
- Task 1.4: Context accumulator stores intent classification + interview responses in .progress.md with parseable format for parameter chain
- Task 1.6: Question pools added with 4 intent types: Trivial (2 questions), Refactor (5), Greenfield (10), Mid-sized (7). Each question marked required/optional with semantic key for storage.
- Task 1.7: POC validation completed via structural code review (context limitations prevent interactive testing). All key patterns verified: Intent Classification (8 occurrences), Question Pools (1 section), Single-Question flow (2 references), Interview Responses (3 references), Question Classification (1 section), AskUserQuestion (6 instances). Created test-adaptive/.progress.md stub demonstrating expected format.
- Task 2.1: Propagated single-question flow to research.md. Key additions: (1) Read Context from .progress.md section to get intent classification and prior answers, (2) Parameter Chain Logic to skip already-answered questions, (3) Single-Question Loop Structure with 4 research-specific questions, (4) Store Research Interview Responses section to append to .progress.md under "### Research Interview".

### Verification: 1.5 [VERIFY] Quality checkpoint: verify start.md is valid markdown
- Status: PASS
- Checks performed:
  - Frontmatter: Valid YAML with opening/closing --- delimiters (lines 1 and 5)
  - Frontmatter fields: description, argument-hint, allowed-tools present
  - Code fences: 72 markers (even = balanced)
  - Headers: 45 markdown headers found
  - Line count: 908 lines total
- Note: Verification command `head -5 | grep -c "^---"` returns 2 (both delimiters in first 5 lines), which is correct for valid frontmatter

## Current Task
Awaiting next task

## Next
Task 2.2 Add single-question flow to requirements.md
